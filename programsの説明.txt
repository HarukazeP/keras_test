github上で表示される辞書順ではなく作成日時順に各プログラムの説明
言語はほとんどがpythonだけどオブジェクト指向に不慣れなため，c言語のような手続き型のような書き方


-----ForCourpusディレクトリ内----------------------------------------

FilePro5.java と WikiSentCheck.java
以前に作成したもの．wp2txtで得たWikipediaの記事データを整形するやつ


process_wiki.py
Wikipediaの記事コーパスを作成
http://tatzyr.hatenablog.com/entry/2015/10/24/162102 このサイトにあったものだが今は記事消えてる


-----2017_from04to07ディレクトリ内----------------------------------------

original.py
kerasのチュートリアルプログラムlstm_text_generation.py そのまま
https://github.com/fchollet/keras/tree/master/examples より


original_time.py
original.py に時間表示を追加


#### これより下「char」で始まるものは文字ベースで予測するモデル．「word」で始まるものは単語ベースで予測するモデル

sampling_test.py
original_time.py に簡単な前処理追加（改行をスペースに，スペースの連続を消す），繰り返し回数減らして軽量化
サンプリングが何をやっているのか確認用


char_sampling2_test.py
サンプリングで複数出力できるように変更，学習データも減らして軽量化


word_test.py
文字ベースから単語ベースへ．さらに前処理を追加．（a～zのみに）
文字ベース時の変数sentenceと単語ベース時の変数sentenceは型が違うことに注意


char_word_test.py
char_sampling2_test.py に前処理を追加．（a～zのみに）
文字べースで予測し，スペースが出るまでを1単語として出力


char_merge_old_test.py
順方向と逆方向から予測してマージする．
Mergeメソッドを用いているが，このMergeは2017/8以降使えなくなるらしい


char_merge_new_test.py
char_merge_old_test.py をMergeからfunctionAPIを用いた方法に変えたもの


word_merge_new_test.py
char_merge_new_test.py の単語ベースver



word_merge_new_wiki_once.py 
※不完全なプログラム
word_merge_new_test.py の学習データをwikiに変えたもの．iterationループをなくした
モデルを保存する機能追加
これではデータが大きすぎて(約6.3GB)動かなかった


word_miniWiki.py
word_merge_new_wiki_once.py から学習データ変えただけ
学習データn行おきに1行抽出して約1.5MBに


word_wiki_line.py
※不完全なプログラム
word_miniWiki.py から学習データが大きい場合に備えて1行ずつ学習していく
AttributeError:'Progbar Logger' object has no attribute 'log_values' みたいなエラーが出てうごかない


word_wiki_not_line.py
word_miniWiki.py から辞書にその他を表す#OTHERを追加
出力ファイル名に日付を付与


word_save_model_goudouzemi2.py
合同ゼミ用
word_wiki_not_line.py から出力とか微妙に変更


word_load_model_goudouzemi2.py
合同ゼミ用
保存してあるモデルを使用して予測
これは実行にそんなに時間かからない


calc_rank_ans.py
合同ゼミ実験1用
word_load_model_goudouzemi2.py が出力したファイルや正解データから正解率を計算


calc_preds_ans.py
合同ゼミ実験2用
word_load_model_goudouzemi2.py が出力したファイルや正解データから正解率を計算


calc_preds_to_rank.py
合同ゼミ実験2用
word_load_model_goudouzemi2.py が出力したファイルや正解データから予測の確率順位を出力




-----2017_from08to11ディレクトリ内----------------------------------------

word_all_after_goudouzemi.py
合同ゼミ用のプログラムでは学習→テスト→正誤判定が分かれていたり，手動部分があったりしたが，それらを一体化し完全自動化
正解付きテストデータと選択肢つきテストデータを読み込む


word_onehot.py
word_all_after_goudouzemi.py とほど同じ
学習回数(epoch)などを変更して実験する


word_embedding.py
word_onehot.py からベクトル化をonehotではなくembedding用いたものへ変更
文字列→単語id+1→embeddingの流れ
embeddingはデフォルトのものを使用しておりword2vecなどは使っていない
embeddingも同時に学習していく


word_embedding_bigfile.py
word_embedding.py を巨大ファイルにも対応させたもの
まず1行ずつ読み取り辞書の作成
その後，1000行ずつに学習していくもの

#### これ以降のプログラムは全てこの巨大ファイル対応形式のもの


word_w2v_test.py
モデルにembedding層を用いるのではなく，モデルの入力をword2vecで作成したベクトルにしたもの
まず学習データでword2vecの学習から始め，それが終わるとニューラルネットの学習へと移る


word_ft_test.py
word_w2v_test.py のfasttext版


word_emb_w2c.py
embeddingの際，word2vecのベクトルを利用し，embedding層は学習を行わないモデル
word_embedding.py とword_w2v_test.py の合体版みたいな


word_emb_ft.py
word_emb_w2c.py のfasttext版


word_ft_loss_graph.py
word_emb_ft.py から変更
学習時の誤差lossとval_lossをグラフ出力


word_ft_output_vec.py
word_ft_loss_graph.py から大きく変更
空所にくる単語の確率分布を予測するモデルから，空所にくる単語の各成分を予測するモデルへ
すなわち，多クラス分類モデルから回帰モデルへ
そのため活性化関数や損失関数を変更


word_ft_class.py
word_ft_loss_graph.py を書き直したもの


word_ft_regression.py
word_ft_output_vec.py を書き直したもの


make_ft_vec.py
gensimのfastteztが生成するベクトルを出力するプログラム
サーバ等でgensimがインストールできないことやバージョン違いの問題に対処するため
12月以降は使っていない

word_ft_class_without_gensim.py
word_ft_class.py のgensimライブラリ使わないバージョン
make_ft_vec.py によって出力されたファイルを利用
12月以降は使っていない








-----2017_from12to03ディレクトリ内----------------------------------------

word_ft_class_devide_line_epoch2.py
word_ft_class.py から大きく変更
1行に全て書かれている巨大データへの対応
様々な処理の関数化
いろいろデバッグ中なので学習データも小さく，epoch=2としている


word_ft_regression_devide_line_epoch2.py
word_ft_regression.py から大きく変更
word_ft_class_devide_line_epoch2.py と同様にいろいろ変えている
いろいろデバッグ中なので学習データも小さく，epoch=2としている


word_ft_regression_e100 シリーズ （word_ft_regression_e100_w10_add_lstm_den1.py など）
word_ft_regression_devide_line_epoch2.py から変更
DEIM用実験はここ
プログラム名が各パラメータの設定
e100  : epoch数．学習回数が100
w10   : 空所の前後何単語を見るか
add   : マージ層での処理
lstm  : 通常のLSTMか，bidirectionalLSTMか
den1  : dence層，つまり全結合層の配置


word_ft_regression_nitsuma.py
word_ft_regression_e100 シリーズと同系
新妻先生から試してみては？と提案された構造のモデル


word_ft_regression_new_e100 シリーズ （word_ft_regression_new_e100_w5_concat_lstm_den1.py など）
word_ft_regression_e100 シリーズから変更
テストデータの変更とテスト部分の関数（prepare_test）を変更


word_ft_regression_new_load_model.py
word_ft_regression_new_e100 シリーズから変更
min_modelディレクトリから学習済みモデルをロードしテストの実行
word_ft_regression_e100 シリーズの再テストなどに利用


word_ft_regression_new_continue_model.py
word_ft_regression_new_e100 シリーズから変更
第二引数のディレクトリからモデルをモードし，学習を再開する
途中で中断したプログラムの再開用
















